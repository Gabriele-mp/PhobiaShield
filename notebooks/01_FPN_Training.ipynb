{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PhobiaShield - FPN Custom Training\n",
        "\n",
        "**Feature Pyramid Network** implementato from-scratch per PhobiaShield.\n",
        "\n",
        "**Team:** Gabriele (Architect) | Member A (Data) | Member C (Demo)\n",
        "\n",
        "**Course:** Fundamentals of Data Science - Sapienza University\n",
        "\n",
        "**Date:** December 2025\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset: ULTIMATE_COMPLETE\n",
        "- **Total:** 11,425 images\n",
        "- **Classes:** 5 (Clown, Shark, Spider, Blood, Needle)\n",
        "- **Split:** 70/15/15\n",
        "- **Size variation:** 260√ó (1.36px to 354px)\n",
        "\n",
        "## Model: PhobiaNetFPN\n",
        "- **Architecture:** Multi-scale FPN (P3, P4, P5)\n",
        "- **Parameters:** 5.4M\n",
        "- **Loss:** Focal Loss + MSE + CrossEntropy\n",
        "\n",
        "## Requirements\n",
        "- GPU: Tesla T4 (16GB) or better\n",
        "- Time: ~2-4 hours for 50 epochs\n",
        "- Drive: Dataset must be in Google Drive"
      ],
      "metadata": {
        "id": "title"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup & Mount Drive"
      ],
      "metadata": {
        "id": "setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "mount"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Clone Repository"
      ],
      "metadata": {
        "id": "clone"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Clone repo\n",
        "!rm -rf /content/PhobiaShield\n",
        "!git clone https://github.com/Gabriele-mp/PhobiaShield.git /content/PhobiaShield\n",
        "\n",
        "# Add to path\n",
        "os.chdir('/content/PhobiaShield')\n",
        "sys.path.insert(0, '/content/PhobiaShield')\n",
        "\n",
        "print(\"‚úÖ Repository cloned\")\n",
        "!ls -la"
      ],
      "metadata": {
        "id": "clone_repo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Install Dependencies"
      ],
      "metadata": {
        "id": "install"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q omegaconf albumentations tqdm\n",
        "print(\"‚úÖ Dependencies installed\")"
      ],
      "metadata": {
        "id": "deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Extract Dataset"
      ],
      "metadata": {
        "id": "dataset"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Paths\n",
        "ZIP_PATH = '/content/drive/MyDrive/PhobiaShield_Models/PhobiaShield/DATASET_ULTIMATE_COMPLETE.zip'\n",
        "EXTRACT_TO = '/content/dataset_ultimate'\n",
        "\n",
        "# Check if zip exists\n",
        "if not os.path.exists(ZIP_PATH):\n",
        "    print(f\"‚ùå Dataset not found at: {ZIP_PATH}\")\n",
        "    print(\"Please upload DATASET_ULTIMATE_COMPLETE.zip to your Drive\")\n",
        "else:\n",
        "    print(f\"üì¶ Extracting dataset...\")\n",
        "    \n",
        "    os.makedirs(EXTRACT_TO, exist_ok=True)\n",
        "    \n",
        "    with zipfile.ZipFile(ZIP_PATH, 'r') as z:\n",
        "        z.extractall(EXTRACT_TO)\n",
        "    \n",
        "    # Count images\n",
        "    train_imgs = len(list(Path(f'{EXTRACT_TO}/train/images').glob('*.jpg')))\n",
        "    val_imgs = len(list(Path(f'{EXTRACT_TO}/val/images').glob('*.jpg')))\n",
        "    test_imgs = len(list(Path(f'{EXTRACT_TO}/test/images').glob('*.jpg')))\n",
        "    \n",
        "    print(f\"‚úÖ Dataset extracted!\")\n",
        "    print(f\"   Train: {train_imgs} images\")\n",
        "    print(f\"   Val: {val_imgs} images\")\n",
        "    print(f\"   Test: {test_imgs} images\")\n",
        "    print(f\"   Total: {train_imgs + val_imgs + test_imgs} images\")"
      ],
      "metadata": {
        "id": "extract"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Load Model & Loss"
      ],
      "metadata": {
        "id": "model"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from src.models.phobia_net_fpn import PhobiaNetFPN\n",
        "from src.models.loss_fpn import FPNLoss\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "# Load config\n",
        "config = OmegaConf.load('cfg/model/tiny_yolo_5class.yaml')\n",
        "config = OmegaConf.to_container(config, resolve=True)\n",
        "\n",
        "# Create model\n",
        "model = PhobiaNetFPN(config, use_attention=True).to('cuda')\n",
        "\n",
        "# Create loss with optimized class weights\n",
        "CLASS_WEIGHTS = [4.76, 1.28, 3.70, 1.01, 1.39]  # Optimized\n",
        "\n",
        "loss_fn = FPNLoss(\n",
        "    num_classes=5,\n",
        "    num_boxes=2,\n",
        "    lambda_coord=5.0,\n",
        "    lambda_obj=5.0,\n",
        "    lambda_noobj=0.05,\n",
        "    use_focal=True,\n",
        "    focal_gamma=2.0,\n",
        "    focal_alpha=0.25,\n",
        "    class_weights=CLASS_WEIGHTS\n",
        ")\n",
        "\n",
        "params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"‚úÖ Model loaded: {params:,} parameters ({params*4/1e6:.2f} MB)\")\n",
        "print(f\"‚úÖ Loss function: FPN Loss with Focal Loss\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Create Datasets"
      ],
      "metadata": {
        "id": "create_dataset"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "\n",
        "class FPNDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Dataset for FPN training\"\"\"\n",
        "    \n",
        "    def __init__(self, img_dir, label_dir, img_size=416, augment=False):\n",
        "        self.img_dir = Path(img_dir)\n",
        "        self.label_dir = Path(label_dir)\n",
        "        self.img_size = img_size\n",
        "        self.img_files = sorted(list(self.img_dir.glob('*.jpg')))\n",
        "        self.grid_sizes = [52, 26, 13]\n",
        "        \n",
        "        if augment:\n",
        "            self.transform = A.Compose([\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                A.RandomBrightnessContrast(p=0.3),\n",
        "            ], bbox_params=A.BboxParams(\n",
        "                format='yolo',\n",
        "                label_fields=['class_labels'],\n",
        "                min_visibility=0.3,\n",
        "                clip=True\n",
        "            ))\n",
        "        else:\n",
        "            self.transform = None\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_files[idx]\n",
        "        img = cv2.imread(str(img_path))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        label_path = self.label_dir / f\"{img_path.stem}.txt\"\n",
        "        \n",
        "        bboxes = []\n",
        "        class_labels = []\n",
        "        \n",
        "        if label_path.exists():\n",
        "            with open(label_path) as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) >= 5:\n",
        "                        cls_id = int(parts[0])\n",
        "                        x, y, w, h = map(float, parts[1:5])\n",
        "                        bboxes.append([x, y, w, h])\n",
        "                        class_labels.append(cls_id)\n",
        "        \n",
        "        if self.transform and bboxes:\n",
        "            try:\n",
        "                transformed = self.transform(image=img, bboxes=bboxes, class_labels=class_labels)\n",
        "                img = transformed['image']\n",
        "                bboxes = transformed['bboxes']\n",
        "                class_labels = transformed['class_labels']\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "        img = cv2.resize(img, (self.img_size, self.img_size))\n",
        "        img = torch.from_numpy(img.transpose(2, 0, 1)).float() / 255.0\n",
        "        \n",
        "        targets = []\n",
        "        \n",
        "        for grid_size in self.grid_sizes:\n",
        "            target = torch.zeros(grid_size, grid_size, 20)\n",
        "            \n",
        "            for bbox, cls_id in zip(bboxes, class_labels):\n",
        "                x, y, w, h = bbox\n",
        "                cls_id = int(cls_id)\n",
        "                \n",
        "                i = min(int(x * grid_size), grid_size - 1)\n",
        "                j = min(int(y * grid_size), grid_size - 1)\n",
        "                \n",
        "                if target[j, i, 4] == 0:\n",
        "                    target[j, i, 0] = x * grid_size - i\n",
        "                    target[j, i, 1] = y * grid_size - j\n",
        "                    target[j, i, 2] = w\n",
        "                    target[j, i, 3] = h\n",
        "                    target[j, i, 4] = 1.0\n",
        "                    target[j, i, 5 + cls_id] = 1.0\n",
        "            \n",
        "            targets.append(target)\n",
        "        \n",
        "        return img, targets\n",
        "    \n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        imgs = torch.stack([b[0] for b in batch])\n",
        "        t_p3 = torch.stack([b[1][0] for b in batch])\n",
        "        t_p4 = torch.stack([b[1][1] for b in batch])\n",
        "        t_p5 = torch.stack([b[1][2] for b in batch])\n",
        "        return imgs, (t_p3, t_p4, t_p5)\n",
        "\n",
        "# Create datasets\n",
        "DATASET_BASE = '/content/dataset_ultimate'\n",
        "\n",
        "train_dataset = FPNDataset(\n",
        "    f'{DATASET_BASE}/train/images',\n",
        "    f'{DATASET_BASE}/train/labels',\n",
        "    augment=True\n",
        ")\n",
        "\n",
        "val_dataset = FPNDataset(\n",
        "    f'{DATASET_BASE}/val/images',\n",
        "    f'{DATASET_BASE}/val/labels',\n",
        "    augment=False\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Train dataset: {len(train_dataset)} images\")\n",
        "print(f\"‚úÖ Val dataset: {len(val_dataset)} images\")"
      ],
      "metadata": {
        "id": "dataset_class"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Training Configuration"
      ],
      "metadata": {
        "id": "train_config"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingWarmRestarts\n",
        "\n",
        "# Training config\n",
        "BATCH = 64\n",
        "EPOCHS = 50\n",
        "LR = 0.000346\n",
        "PATIENCE = 20\n",
        "\n",
        "# Create loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH,\n",
        "    shuffle=True,\n",
        "    collate_fn=FPNDataset.collate_fn,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH,\n",
        "    shuffle=False,\n",
        "    collate_fn=FPNDataset.collate_fn,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=LR,\n",
        "    weight_decay=0.0001\n",
        ")\n",
        "\n",
        "# Schedulers\n",
        "warmup_epochs = 10\n",
        "\n",
        "def warmup_lambda(epoch):\n",
        "    if epoch < warmup_epochs:\n",
        "        return (epoch + 1) / warmup_epochs\n",
        "    return 1.0\n",
        "\n",
        "warmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup_lambda)\n",
        "cosine_scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=15)\n",
        "\n",
        "# Mixed precision\n",
        "scaler = GradScaler('cuda')\n",
        "\n",
        "# Save directory\n",
        "SAVE_DIR = '/content/drive/MyDrive/PhobiaShield_Models/fpn_custom'\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Training config:\")\n",
        "print(f\"   Batch: {BATCH}\")\n",
        "print(f\"   Epochs: {EPOCHS}\")\n",
        "print(f\"   LR: {LR}\")\n",
        "print(f\"   Train batches: {len(train_loader)}\")\n",
        "print(f\"   Val batches: {len(val_loader)}\")"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Training Loop"
      ],
      "metadata": {
        "id": "training"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üöÄ STARTING FPN TRAINING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_start = time.time()\n",
        "    \n",
        "    # TRAIN\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    \n",
        "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\")\n",
        "    \n",
        "    for imgs, targets in train_bar:\n",
        "        imgs = imgs.to('cuda', non_blocking=True)\n",
        "        targets = tuple(t.to('cuda', non_blocking=True) for t in targets)\n",
        "        \n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        \n",
        "        with autocast('cuda'):\n",
        "            preds = model(imgs)\n",
        "            loss, _ = loss_fn(preds, targets)\n",
        "        \n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "        train_bar.set_postfix({'loss': f\"{loss.item():.3f}\"})\n",
        "    \n",
        "    train_loss /= len(train_loader)\n",
        "    \n",
        "    # VAL\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\")\n",
        "        \n",
        "        for imgs, targets in val_bar:\n",
        "            imgs = imgs.to('cuda', non_blocking=True)\n",
        "            targets = tuple(t.to('cuda', non_blocking=True) for t in targets)\n",
        "            \n",
        "            with autocast('cuda'):\n",
        "                preds = model(imgs)\n",
        "                loss, _ = loss_fn(preds, targets)\n",
        "            \n",
        "            val_loss += loss.item()\n",
        "            val_bar.set_postfix({'loss': f\"{loss.item():.3f}\"})\n",
        "    \n",
        "    val_loss /= len(val_loader)\n",
        "    \n",
        "    # Scheduler\n",
        "    if epoch < warmup_epochs:\n",
        "        warmup_scheduler.step()\n",
        "    else:\n",
        "        cosine_scheduler.step()\n",
        "    \n",
        "    epoch_time = (time.time() - epoch_start) / 60\n",
        "    \n",
        "    # Log\n",
        "    print(f\"\\nüìä Epoch {epoch+1}/{EPOCHS}\")\n",
        "    print(f\"   Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
        "    print(f\"   Time: {epoch_time:.1f} min\")\n",
        "    print(f\"   LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "    \n",
        "    # Save best\n",
        "    if val_loss < best_val_loss:\n",
        "        improvement = best_val_loss - val_loss\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        \n",
        "        checkpoint_path = f'{SAVE_DIR}/fpn_best_e{epoch+1}_loss{val_loss:.4f}.pth'\n",
        "        \n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': val_loss,\n",
        "        }, checkpoint_path)\n",
        "        \n",
        "        print(f\"   ‚úÖ Best! Saved (‚Üì {improvement:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"   ‚ö†Ô∏è  No improvement ({patience_counter}/{PATIENCE})\")\n",
        "        \n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(f\"\\nüõë Early stopping\")\n",
        "            break\n",
        "    \n",
        "    print()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ TRAINING COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
        "print(f\"Model saved in: {SAVE_DIR}\")"
      ],
      "metadata": {
        "id": "train_loop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Results Summary"
      ],
      "metadata": {
        "id": "results"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüìä TRAINING SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n‚úÖ Best Model:\")\n",
        "print(f\"   Validation Loss: {best_val_loss:.4f}\")\n",
        "print(f\"   Saved at: {SAVE_DIR}\")\n",
        "print(f\"\\nüìÅ Checkpoint files:\")\n",
        "!ls -lh \"$SAVE_DIR\"\n",
        "\n",
        "print(\"\\nüéØ Next Steps:\")\n",
        "print(\"   1. Run evaluation notebook (03_Evaluation.ipynb)\")\n",
        "print(\"   2. Compare with YOLOv8 results\")\n",
        "print(\"   3. Generate metrics and confusion matrix\")"
      ],
      "metadata": {
        "id": "summary"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
